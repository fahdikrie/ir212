{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Referensi\n",
    "1. https://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/\n",
    "2. https://www.datacamp.com/community/tutorials/fuzzy-string-python"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "source": [
    "# Mengimpor library & package yang digunakan\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "import stanza\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import display\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **A - Edit Distance (40 Poin)**\n",
    "\n",
    "### 1. [10] Buatlah sebuah fungsi “edit_distance”. Fungsi tersebut memiliki dua buah parameter string yaitu string_1 dan string_2. Fungsi ini dapat menghitung Levenshtein Distance antara string_1 dengan string_2. Lebih jelasnya lagi, fungsi tersebut akan mengembalikan nilai terkecil yang dibutuhkan untuk mentransformasi string_1 menjadi string_2."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "source": [
    "def edit_distance(string_1, string_2):\n",
    "    \"\"\"\n",
    "    Reference: https://www.datacamp.com/community/tutorials/fuzzy-string-python\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize matrix of zeros\n",
    "    rows = len(string_1)+1\n",
    "    cols = len(string_2)+1\n",
    "    distances = np.zeros((rows,cols),dtype = int)\n",
    "\n",
    "    # Populate matrix of zeros with the indeces of each character of both strings\n",
    "    for i in range(1, rows):\n",
    "        for k in range(1,cols):\n",
    "            distances[i][0] = i\n",
    "            distances[0][k] = k\n",
    "\n",
    "    # Iterate over the matrix to compute the cost of deletions,insertions and/or substitutions\n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if string_1[row-1] == string_2[col-1]:\n",
    "                cost = 0 # If the characters are the same in the two strings in a given position [i,j] then the cost is 0\n",
    "            else:\n",
    "                cost = 1\n",
    "\n",
    "            distances[row][col] = min(\n",
    "                distances[row-1][col] + 1,       # Cost of deletions\n",
    "                distances[row][col-1] + 1,       # Cost of insertions\n",
    "                distances[row-1][col-1] + cost   # Cost of substitutions\n",
    "            )\n",
    "\n",
    "    pprint(distances)\n",
    "\n",
    "    return distances[row][col]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. [5] Menggunakan fungsi yang telah dibuat pada soal sebelumnya, carilah nilai edit_distance dari pasangan kata berikut ini:\n",
    "\n",
    "### a. phasmophobia - puafnilhotik"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "source": [
    "edit_distance(\"phasmophobia\", \"puafnilhotik\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "       [ 1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
      "       [ 2,  1,  1,  2,  3,  4,  5,  6,  6,  7,  8,  9, 10],\n",
      "       [ 3,  2,  2,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
      "       [ 4,  3,  3,  2,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
      "       [ 5,  4,  4,  3,  3,  3,  4,  5,  6,  7,  8,  9, 10],\n",
      "       [ 6,  5,  5,  4,  4,  4,  4,  5,  6,  6,  7,  8,  9],\n",
      "       [ 7,  6,  6,  5,  5,  5,  5,  5,  6,  7,  7,  8,  9],\n",
      "       [ 8,  7,  7,  6,  6,  6,  6,  6,  5,  6,  7,  8,  9],\n",
      "       [ 9,  8,  8,  7,  7,  7,  7,  7,  6,  5,  6,  7,  8],\n",
      "       [10,  9,  9,  8,  8,  8,  8,  8,  7,  6,  6,  7,  8],\n",
      "       [11, 10, 10,  9,  9,  9,  8,  9,  8,  7,  7,  6,  7],\n",
      "       [12, 11, 11, 10, 10, 10,  9,  9,  9,  8,  8,  7,  7]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "metadata": {},
     "execution_count": 585
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b. genetik - ganteng"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "source": [
    "edit_distance(\"genetik\", \"ganteng\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "array([[0, 1, 2, 3, 4, 5, 6, 7],\n",
      "       [1, 0, 1, 2, 3, 4, 5, 6],\n",
      "       [2, 1, 1, 2, 3, 3, 4, 5],\n",
      "       [3, 2, 2, 1, 2, 3, 3, 4],\n",
      "       [4, 3, 3, 2, 2, 2, 3, 4],\n",
      "       [5, 4, 4, 3, 2, 3, 3, 4],\n",
      "       [6, 5, 5, 4, 3, 3, 4, 4],\n",
      "       [7, 6, 6, 5, 4, 4, 4, 5]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "execution_count": 586
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. [13] Salah satu metode untuk menghitung edit distance secara heuristik adalah menggunakan “keyboard distance”. Penentuan “bobot” atau “weight” pada metode ini dapat bervariasi. Penggunaan graf dari keyboard (asumsikan QWERTY keyboard) merupakan salah satu metode yang dapat dilakukan.\n",
    "\n",
    "### Pada graf yang akan digunakan, satu node merepresentasikan satu key. Kemudian, satu edge menandakan bahwa dua key berdekatan yang artinya memiliki bobot edit distance sebesar 1.\n",
    "\n",
    "### Buatlah matriks edit distance M dari suatu QWERTY keyboard! M(ii, jj) menyatakan jarak terpendek antara huruf “ii” ke huruf “jj” dalam graf QWERTY keyboard."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "source": [
    "qwerty_keyboard = [\n",
    "    ['q', 'w', 'e', 'r', 't', 'y', 'u', 'i', 'o', 'p'],\n",
    "    ['a', 's', 'd', 'f', 'g', 'h', 'j', 'k', 'l'],\n",
    "    [' ', 'z', 'x', 'c', 'v', 'b', 'n', 'm', ' '],\n",
    "]\n",
    "\n",
    "alphabet = list(string.ascii_lowercase)\n",
    "\n",
    "\n",
    "def getCharacterCoord(char, array):\n",
    "    ii, jj = -1, -1\n",
    "    for row in array:\n",
    "        if char in row:\n",
    "            ii = array.index(row)\n",
    "            jj = row.index(char)\n",
    "            return (ii, jj)\n",
    "    raise ValueError(char + \" not found in given keyboard layout\")\n",
    "\n",
    "def euclideanKeyboardDistance(c1, c2):\n",
    "    coord1 = getCharacterCoord(c1, qwerty_keyboard)\n",
    "    coord2 = getCharacterCoord(c2, qwerty_keyboard)\n",
    "    return round(((coord1[0] - coord2[0])**2 + (coord1[1] - coord2[1])**2)**(0.5))\n",
    "\n",
    "def M(ii, jj):\n",
    "    qwerty_keyboard = [\n",
    "        []\n",
    "    ]\n",
    "\n",
    "edit_distance_sum = 0\n",
    "\n",
    "w1 = 'phasmophobia'\n",
    "w2 = 'puafnilhotik'\n",
    "# w1 = 'genetik'\n",
    "# w2 = 'ganteng'\n",
    "\n",
    "for i in range(len(w1)):\n",
    "    distance = euclideanKeyboardDistance(w1[i], w2[i])\n",
    "    # print(distance)\n",
    "    if distance:\n",
    "        edit_distance_sum += distance\n",
    "\n",
    "print(edit_distance_sum)\n",
    "print(euclideanKeyboardDistance('p', 'z'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15\n",
      "8\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. [5] Menggunakan matriks M yang telah dibuat pada soal sebelumnya, carilah nilai edit_distance dari pasangan kata berikut ini:\n",
    "\n",
    "### a. phasmophobia - puafnilhotik"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b. genetik - ganteng"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. [7] Bandingkan hasil yang kamu temui pada nomor A2 dan A4. Apa yang dapat kamu simpulkan terkait kedua metode edit distance ini? Selain itu, apakah kedua algoritma edit distance ini sudah sempurna? Jelaskan analisis singkat kamu minimal dalam 3 kalimat."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **B - N-Gram Language Model (30 Poin)**\n",
    "\n",
    "### Perhatikan empat kalimat berikut\n",
    "### K1 : orang itu menjadi kepala rumah sakit di desa\n",
    "### K2 : kepala orang itu sedang sakit di rumah sakit\n",
    "### K3 : kepala rumah sakit itu memakan kelapa bersama warga desa\n",
    "### K4 : orang sakit itu memakan kelapa bersama kepala rumah sakit\n",
    "\n",
    "### 1. [12] Jika diberikan threshold sebesar 10^-12 dengan menggunakan language model bigram dan trigram, manakah di antara kalimat berikut ini yang memiliki kecenderungan pada spelling error? Tunjukkan peluang dari setiap kalimat berikut dan tentukan kalimat yang mengandung spelling error!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "source": [
    "sentences = [\n",
    "    'orang itu menjadi kepala rumah sakit di desa',\n",
    "    'kepala orang itu sedang sakit di rumah sakit',\n",
    "    'kepala rumah sakit itu memakan kelapa bersama warga desa',\n",
    "    'orang sakit itu memakan kelapa bersama kepala rumah sakit',\n",
    "]\n",
    "\n",
    "unigrams = [\n",
    "    row.split(' ') for row in sentences\n",
    "]\n",
    "\n",
    "bigrams = map(\n",
    "    lambda row: list(nltk.ngrams(row.split(' '), 2)),\n",
    "    sentences\n",
    ")\n",
    "\n",
    "trigrams = map(\n",
    "    lambda row: list(nltk.ngrams(row.split(' '), 3)),\n",
    "    sentences\n",
    ")\n",
    "\n",
    "unigrams_sr = pd.Series(unigrams)\n",
    "bigrams_sr = pd.Series(bigrams)\n",
    "trigrams_sr = pd.Series(trigrams)\n",
    "\n",
    "print(list(unigrams_sr), \"\\n\")\n",
    "print(list(bigrams_sr), \"\\n\")\n",
    "print(list(trigrams_sr), \"\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['orang', 'itu', 'menjadi', 'kepala', 'rumah', 'sakit', 'di', 'desa'], ['kepala', 'orang', 'itu', 'sedang', 'sakit', 'di', 'rumah', 'sakit'], ['kepala', 'rumah', 'sakit', 'itu', 'memakan', 'kelapa', 'bersama', 'warga', 'desa'], ['orang', 'sakit', 'itu', 'memakan', 'kelapa', 'bersama', 'kepala', 'rumah', 'sakit']] \n",
      "\n",
      "[[('orang', 'itu'), ('itu', 'menjadi'), ('menjadi', 'kepala'), ('kepala', 'rumah'), ('rumah', 'sakit'), ('sakit', 'di'), ('di', 'desa')], [('kepala', 'orang'), ('orang', 'itu'), ('itu', 'sedang'), ('sedang', 'sakit'), ('sakit', 'di'), ('di', 'rumah'), ('rumah', 'sakit')], [('kepala', 'rumah'), ('rumah', 'sakit'), ('sakit', 'itu'), ('itu', 'memakan'), ('memakan', 'kelapa'), ('kelapa', 'bersama'), ('bersama', 'warga'), ('warga', 'desa')], [('orang', 'sakit'), ('sakit', 'itu'), ('itu', 'memakan'), ('memakan', 'kelapa'), ('kelapa', 'bersama'), ('bersama', 'kepala'), ('kepala', 'rumah'), ('rumah', 'sakit')]] \n",
      "\n",
      "[[('orang', 'itu', 'menjadi'), ('itu', 'menjadi', 'kepala'), ('menjadi', 'kepala', 'rumah'), ('kepala', 'rumah', 'sakit'), ('rumah', 'sakit', 'di'), ('sakit', 'di', 'desa')], [('kepala', 'orang', 'itu'), ('orang', 'itu', 'sedang'), ('itu', 'sedang', 'sakit'), ('sedang', 'sakit', 'di'), ('sakit', 'di', 'rumah'), ('di', 'rumah', 'sakit')], [('kepala', 'rumah', 'sakit'), ('rumah', 'sakit', 'itu'), ('sakit', 'itu', 'memakan'), ('itu', 'memakan', 'kelapa'), ('memakan', 'kelapa', 'bersama'), ('kelapa', 'bersama', 'warga'), ('bersama', 'warga', 'desa')], [('orang', 'sakit', 'itu'), ('sakit', 'itu', 'memakan'), ('itu', 'memakan', 'kelapa'), ('memakan', 'kelapa', 'bersama'), ('kelapa', 'bersama', 'kepala'), ('bersama', 'kepala', 'rumah'), ('kepala', 'rumah', 'sakit')]] \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "source": [
    "unigrams_sr_explode = unigrams_sr.explode()\n",
    "unigrams_sr_value_counts = unigrams_sr_explode.value_counts()\n",
    "\n",
    "bigrams_sr_explode = bigrams_sr.explode()\n",
    "bigrams_sr_value_counts = bigrams_sr_explode.value_counts()\n",
    "\n",
    "trigrams_sr_explode = trigrams_sr.explode()\n",
    "trigrams_sr_value_counts = trigrams_sr_explode.value_counts()\n",
    "\n",
    "# print(unigrams_sr_explode, \"\\n\")\n",
    "# print(bigrams_sr_explode, \"\\n\")\n",
    "# print(trigrams_sr_explode, \"\\n\")\n",
    "print(unigrams_sr_value_counts, \"\\n\")\n",
    "print(bigrams_sr_value_counts, \"\\n\")\n",
    "print(trigrams_sr_value_counts, \"\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sakit      6\n",
      "itu        4\n",
      "kepala     4\n",
      "rumah      4\n",
      "orang      3\n",
      "di         2\n",
      "desa       2\n",
      "memakan    2\n",
      "kelapa     2\n",
      "bersama    2\n",
      "menjadi    1\n",
      "sedang     1\n",
      "warga      1\n",
      "dtype: int64 \n",
      "\n",
      "(rumah, sakit)       4\n",
      "(kepala, rumah)      3\n",
      "(orang, itu)         2\n",
      "(sakit, itu)         2\n",
      "(sakit, di)          2\n",
      "(kelapa, bersama)    2\n",
      "(memakan, kelapa)    2\n",
      "(itu, memakan)       2\n",
      "(orang, sakit)       1\n",
      "(warga, desa)        1\n",
      "(bersama, warga)     1\n",
      "(sedang, sakit)      1\n",
      "(di, rumah)          1\n",
      "(itu, menjadi)       1\n",
      "(itu, sedang)        1\n",
      "(kepala, orang)      1\n",
      "(di, desa)           1\n",
      "(menjadi, kepala)    1\n",
      "(bersama, kepala)    1\n",
      "dtype: int64 \n",
      "\n",
      "(kepala, rumah, sakit)        3\n",
      "(memakan, kelapa, bersama)    2\n",
      "(itu, memakan, kelapa)        2\n",
      "(sakit, itu, memakan)         2\n",
      "(orang, itu, menjadi)         1\n",
      "(di, rumah, sakit)            1\n",
      "(kelapa, bersama, kepala)     1\n",
      "(orang, sakit, itu)           1\n",
      "(bersama, warga, desa)        1\n",
      "(kelapa, bersama, warga)      1\n",
      "(rumah, sakit, itu)           1\n",
      "(sakit, di, rumah)            1\n",
      "(itu, menjadi, kepala)        1\n",
      "(sedang, sakit, di)           1\n",
      "(itu, sedang, sakit)          1\n",
      "(orang, itu, sedang)          1\n",
      "(kepala, orang, itu)          1\n",
      "(sakit, di, desa)             1\n",
      "(rumah, sakit, di)            1\n",
      "(menjadi, kepala, rumah)      1\n",
      "(bersama, kepala, rumah)      1\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "source": [
    "def count_occurence(tup_src, model):\n",
    "    \"\"\"\n",
    "    Count occurences of a n-gram in the model provided\n",
    "\n",
    "    params\n",
    "    tup_src: tuple n-gram yang ingin dicek\n",
    "    model: model yang menjadi referensi\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return model[tup_src]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def count_bigrams_prob(bigrams):\n",
    "    \"\"\"\n",
    "    Count bigrams probability relative to the model di atas\n",
    "\n",
    "    params\n",
    "    bigrams: bigram yang pengen diitung prob-nya\n",
    "    \"\"\"\n",
    "    bigrams_prob = (\n",
    "        count_occurence(bigrams_a[0][0], unigrams_sr_value_counts)/\n",
    "        unigrams_sr_value_counts.size\n",
    "    )\n",
    "\n",
    "    for bigram in bigrams:\n",
    "        count_bigram = count_occurence(bigram, bigrams_sr_value_counts)\n",
    "        count_word = count_occurence(bigram[0], unigrams_sr_value_counts)\n",
    "        count_prob = (count_bigram + 1)/(count_word + len(bigram[0]))\n",
    "\n",
    "        if count_prob:\n",
    "            bigrams_prob *= count_prob\n",
    "\n",
    "    return bigrams_prob\n",
    "\n",
    "def count_trigrams_prob(trigrams):\n",
    "    \"\"\"\n",
    "    Count trigrams probability relative to the model di atas\n",
    "\n",
    "    params\n",
    "    trigrams: trigram yang pengen diitung prob-nya\n",
    "    \"\"\"\n",
    "    trigrams_prob = (\n",
    "        count_occurence(trigrams_a[0][0], unigrams_sr_value_counts)/\n",
    "        unigrams_sr_value_counts.size\n",
    "    )\n",
    "\n",
    "    for trigram in trigrams:\n",
    "        count_trigram = count_occurence(trigram, trigrams_sr_value_counts)\n",
    "        count_word = count_occurence(trigram[0], unigrams_sr_value_counts)\n",
    "        count_prob = (count_trigram + 1)/(count_word + len(trigram[0]))\n",
    "\n",
    "        if count_prob:\n",
    "            trigrams_prob *= count_prob\n",
    "\n",
    "    return trigrams_prob\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a. warga desa sedang sakit kelapa di rumah sakit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "source": [
    "sentence_a = 'warga desa sedang sakit kelapa di rumah sakit'\n",
    "\n",
    "bigrams_a = list(nltk.ngrams(sentence_a.split(' '), 2))\n",
    "trigrams_a = list(nltk.ngrams(sentence_a.split(' '), 3))\n",
    "\n",
    "bigrams_prob = count_bigrams_prob(bigrams_a)\n",
    "trigrams_prob = count_trigrams_prob(bigrams_a)\n",
    "\n",
    "print(bigrams_prob)\n",
    "print(trigrams_prob)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.854170520837187e-06\n",
      "9.635426302092967e-08\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "b. orang itu sedang berada di rumah sakit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "source": [
    "sentence_b = 'orang itu sedang berada di rumah sakit'\n",
    "\n",
    "bigrams_b = list(nltk.ngrams(sentence_b.split(' '), 2))\n",
    "trigrams_b = list(nltk.ngrams(sentence_b.split(' '), 3))\n",
    "\n",
    "bigrams_prob = count_bigrams_prob(bigrams_b)\n",
    "trigrams_prob = count_trigrams_prob(bigrams_b)\n",
    "\n",
    "print(bigrams_prob)\n",
    "print(trigrams_prob)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5.450898308041165e-05\n",
      "9.084830513401942e-07\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "c. rumah sakit di desa itu tempat warga memakan kepala"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "source": [
    "sentence_c = 'rumah sakit di desa itu tempat warga memakan kepala'\n",
    "\n",
    "bigrams_c = list(nltk.ngrams(sentence_c.split(' '), 2))\n",
    "trigrams_c = list(nltk.ngrams(sentence_c.split(' '), 3))\n",
    "\n",
    "bigrams_prob = count_bigrams_prob(bigrams_c)\n",
    "trigrams_prob = count_trigrams_prob(bigrams_c)\n",
    "\n",
    "print(bigrams_prob)\n",
    "print(trigrams_prob)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4.282411689819096e-07\n",
      "1.4274705632730318e-08\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "d. orang sakit sedang memakan kelapa bersama warga desa"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "source": [
    "sentence_d = 'orang sakit sedang memakan kelapa bersama warga desa'\n",
    "\n",
    "bigrams_d = list(nltk.ngrams(sentence_d.split(' '), 2))\n",
    "trigrams_d = list(nltk.ngrams(sentence_d.split(' '), 3))\n",
    "\n",
    "bigrams_prob = count_bigrams_prob(bigrams_d)\n",
    "trigrams_prob = count_trigrams_prob(bigrams_d)\n",
    "\n",
    "print(bigrams_prob)\n",
    "print(trigrams_prob)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.3125023125023124e-06\n",
      "3.2118087673643226e-08\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. [8] Dalam kasus umum (tidak terbatas pada soal ini), menurut kamu, manakah yang lebih baik; language model unigram, bigram, atau trigram? Berikan alasan pendukung jawaban kamu minimal dalam 3 kalimat."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. [10] Menurut kamu apakah N-gram language model yang standar sudah baik untuk diterapkan pada bahasa yang kaya secara morfologis, seperti Bahasa Indonesia? Apa saja masalah berkaitan dengan morfologis yang mungkin ditemukan pada kasus ini? Ceritakan strategi kamu dalam mengatasinya dikaitkan dengan pengetahuan yang telah didapatkan pada bagian Text Processing minimal dalam 3 kalimat."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **C - Pemetaan Dokumen dan Teks (30 Poin)**\n",
    "\n",
    "### Dokumen A\n",
    "<blockquote>\n",
    "Dokumen A Sivitas Akademika Universitas Indonesia menjalankan aktivitas perkuliahan secara daring yang diikuti oleh mahasiswa dari seluruh jenjang. Hal ini diakibatkan karena adanya pandemi COVID-19 di Indonesia selama satu setengah tahun belakang.\n",
    "</blockquote>\n",
    "\n",
    "### Dokumen B\n",
    "<blockquote>\n",
    "Masyarakat memiliki respon berbeda dalam menyikapi pandemi COVID-19. Salah satu mahasiswa bernama Dipsy mengatakan kesulitan untuk belajar dengan maksimal karena sulit untuk berdiskusi dengan orang yang lebih pandai.\n",
    "</blockquote>\n",
    "\n",
    "### Dokumen C\n",
    "<blockquote>\n",
    "Salah satu kunci untuk menghadapi musibah adalah dengan bersabar dan yakin semua musibah akan berlalu pada waktunya. Badai pasti berlalu, jadi harus tetap semangat.\n",
    "</blockquote>\n",
    "\n",
    "### 1. [10] Buatlah pemetaan seluruh kata yang sudah dalam bentuk kata dasar lowercase dan bukan termasuk dalam stopwords dengan jumlah kemunculan kata tersebut di setiap dokumen. Kamu dibebaskan bagaimana cara menampilkannya, (tidak harus dalam bentuk tabel, bisa saja dalam bentuk key-value pair)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "source": [
    "documents = [\n",
    "    'Dokumen A Sivitas Akademika Universitas Indonesia menjalankan aktivitas perkuliahan secara daring yang diikuti oleh mahasiswa dari seluruh jenjang. Hal ini diakibatkan karena adanya pandemi COVID-19 di Indonesia selama satu setengah tahun belakang.',\n",
    "    'Masyarakat memiliki respon berbeda dalam menyikapi pandemi COVID-19. Salah satu mahasiswa bernama Dipsy mengatakan kesulitan untuk belajar dengan maksimal karena sulit untuk berdiskusi dengan orang yang lebih pandai.',\n",
    "    'Salah satu kunci untuk menghadapi musibah adalah dengan bersabar dan yakin semua musibah akan berlalu pada waktunya. Badai pasti berlalu, jadi harus tetap semangat.',\n",
    "]\n",
    "\n",
    "documents_sr = pd.Series(documents)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "source": [
    "# Perform uncasing\n",
    "cleaned_documents = documents_sr.str.lower()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "source": [
    "# Tokenize documents\n",
    "cleaned_documents_tokenized = cleaned_documents.map(\n",
    "    lambda document: nltk.word_tokenize(document)\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "source": [
    "# Remove stopwords\n",
    "stopwords = pd.read_csv('stopwords.txt', header=None)\n",
    "\n",
    "cleaned_documents_tokenized_no_sw = cleaned_documents_tokenized.copy(deep=True)\n",
    "\n",
    "for index, document in cleaned_documents_tokenized_no_sw.iteritems():\n",
    "    cleaned_documents_tokenized_no_sw.iloc[index] = list(\n",
    "        filter(\n",
    "            lambda word: word not in stopwords.values,\n",
    "            document\n",
    "        )\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "source": [
    "# Menampikan dokumen yang sudah ditokenisasi dan diproses\n",
    "pprint(cleaned_documents_tokenized_no_sw[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['dokumen',\n",
      " 'a',\n",
      " 'sivitas',\n",
      " 'akademika',\n",
      " 'universitas',\n",
      " 'indonesia',\n",
      " 'menjalankan',\n",
      " 'aktivitas',\n",
      " 'perkuliahan',\n",
      " 'daring',\n",
      " 'diikuti',\n",
      " 'mahasiswa',\n",
      " 'jenjang',\n",
      " '.',\n",
      " 'diakibatkan',\n",
      " 'pandemi',\n",
      " 'covid-19',\n",
      " 'indonesia',\n",
      " '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "source": [
    "pprint(cleaned_documents_tokenized_no_sw[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['masyarakat',\n",
      " 'memiliki',\n",
      " 'respon',\n",
      " 'berbeda',\n",
      " 'menyikapi',\n",
      " 'pandemi',\n",
      " 'covid-19',\n",
      " '.',\n",
      " 'salah',\n",
      " 'mahasiswa',\n",
      " 'bernama',\n",
      " 'dipsy',\n",
      " 'kesulitan',\n",
      " 'belajar',\n",
      " 'maksimal',\n",
      " 'sulit',\n",
      " 'berdiskusi',\n",
      " 'orang',\n",
      " 'pandai',\n",
      " '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "source": [
    "pprint(cleaned_documents_tokenized_no_sw[2])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['salah',\n",
      " 'kunci',\n",
      " 'menghadapi',\n",
      " 'musibah',\n",
      " 'bersabar',\n",
      " 'musibah',\n",
      " '.',\n",
      " 'badai',\n",
      " ',',\n",
      " 'semangat',\n",
      " '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. [10] Jika menjalankan masing-masing query berikut, dokumen manakah yang akan ditemukan?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "source": [
    "DOCUMENTS_INDEX_DICT = {\n",
    "    0: 'A',\n",
    "    1: 'B',\n",
    "    2: 'C',\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a. akibat AND sulit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "source": [
    "doc_index = 0\n",
    "is_result_found = False\n",
    "\n",
    "for document in cleaned_documents_tokenized_no_sw:\n",
    "    if (('akibat' in document) and ('sulit' in document)):\n",
    "        print(f'Dokumen {DOCUMENTS_INDEX_DICT[doc_index]}')\n",
    "        is_result_found = True\n",
    "    doc_index += 1\n",
    "\n",
    "if not is_result_found:\n",
    "    print(\"Query tidak ditemukan dari dokumen manapun\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Query tidak ditemukan dari dokumen manapun\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b. waktu OR santai"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "source": [
    "doc_index = 0\n",
    "is_result_found = False\n",
    "\n",
    "for document in cleaned_documents_tokenized_no_sw:\n",
    "    if (('waktu' in document) or ('santai' in document)):\n",
    "        print(f'Dokumen {DOCUMENTS_INDEX_DICT[doc_index]}')\n",
    "        is_result_found = True\n",
    "    doc_index += 1\n",
    "\n",
    "if not is_result_found:\n",
    "    print(\"Query tidak ditemukan dari dokumen manapun\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Query tidak ditemukan dari dokumen manapun\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c. (masyarakat AND mahasiswa) OR sabar"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "source": [
    "doc_index = 0\n",
    "is_result_found = False\n",
    "\n",
    "for document in cleaned_documents_tokenized_no_sw:\n",
    "    if ((('masyarakat' in document) and ('mahasiswa' in document)) or ('sabar') in document):\n",
    "        print(f'Dokumen {DOCUMENTS_INDEX_DICT[doc_index]}')\n",
    "        is_result_found = True\n",
    "    doc_index += 1\n",
    "\n",
    "if not is_result_found:\n",
    "    print(\"Query tidak ditemukan dari dokumen manapun\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dokumen B\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### d. (libur OR respon) OR (waktu AND jenjang)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "source": [
    "doc_index = 0\n",
    "is_result_found = False\n",
    "\n",
    "for document in cleaned_documents_tokenized_no_sw:\n",
    "    if ((('libur' in document) or ('respon' in document)) or\n",
    "        (('waktu') in document) and ('jenjang' in document)):\n",
    "        print(f'Dokumen {DOCUMENTS_INDEX_DICT[doc_index]}')\n",
    "        is_result_found = True\n",
    "    doc_index += 1\n",
    "\n",
    "if not is_result_found:\n",
    "    print(\"Query tidak ditemukan dari dokumen manapun\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dokumen B\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### e. (kunci OR musibah) AND NOT (aktivitas AND pandemi) OR maksimal"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "source": [
    "doc_index = 0\n",
    "is_result_found = False\n",
    "\n",
    "for document in cleaned_documents_tokenized_no_sw:\n",
    "    if ((('kunci' in document) or ('musibah' in document)) and not\n",
    "        (('aktivitas') in document) and ('pandemi' in document) or\n",
    "        ('maksimal') in document):\n",
    "        print(f'Dokumen {DOCUMENTS_INDEX_DICT[doc_index]}')\n",
    "        is_result_found = True\n",
    "    doc_index += 1\n",
    "\n",
    "if not is_result_found:\n",
    "    print(\"Query tidak ditemukan dari dokumen manapun\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dokumen B\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. [10] Apakah metode pemetaan dokumen dan teks dapat memberikan manfaat dalam proses perancangan sistem IR apabila dilihat dari perspektif keakuratan hasil yang akan dihasilkan? Jelaskan analisis singkat kamu minimal dalam 3 kalimat."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}