{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referensi\n",
    "1. https://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/\n",
    "2. https://www.datacamp.com/community/tutorials/fuzzy-string-python\n",
    "3. https://medium.com/swlh/language-modelling-with-nltk-20eac7e70853\n",
    "4. https://github.com/wsong/Typo-Distance/blob/master/typodistance.py\n",
    "5. https://web.stanford.edu/~jurafsky/slp3/slides/3_LM_Jan_08_2021.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atribusi:\n",
    "1. Berkolaborasi dengan Fairuza Raryasdya untuk menanyakan logika dibalik pengerjaan section B\n",
    "2. Berkolaborasi dengan Intan Fadilla untuk saling memastikan asumsi terkait pengerjaan nomor A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengimpor library & package yang digunakan\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **A - Edit Distance (40 Poin)**\n",
    "\n",
    "### 1. [10] Buatlah sebuah fungsi “edit_distance”. Fungsi tersebut memiliki dua buah parameter string yaitu string_1 dan string_2. Fungsi ini dapat menghitung Levenshtein Distance antara string_1 dengan string_2. Lebih jelasnya lagi, fungsi tersebut akan mengembalikan nilai terkecil yang dibutuhkan untuk mentransformasi string_1 menjadi string_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(string_1, string_2):\n",
    "    \"\"\"\n",
    "    Fungsi edit_distance yang mengembalikan hasil perhitungan jarak edit\n",
    "    antara dua string dengan menggunakan algoritma Leivenshtein Distance\n",
    "\n",
    "    Reference: https://www.datacamp.com/community/tutorials/fuzzy-string-python\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize matrix of zeros\n",
    "    rows = len(string_1) + 1\n",
    "    cols = len(string_2) + 1\n",
    "    distances = np.zeros((rows, cols), dtype = int)\n",
    "\n",
    "    # Populate matrix of zeros with the indeces of each character of both strings\n",
    "    for i in range(1, rows):\n",
    "        for k in range(1, cols):\n",
    "            distances[i][0] = i\n",
    "            distances[0][k] = k\n",
    "\n",
    "    # Iterate over the matrix to compute the cost of deletions,insertions and/or substitutions\n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if string_1[row - 1] == string_2[col - 1]:\n",
    "                cost = 0 # If the characters are the same in the two strings in a given position [i,j] then the cost is 0\n",
    "            else:\n",
    "                cost = 1\n",
    "\n",
    "            distances[row][col] = min(\n",
    "                distances[row - 1][col] + 1,         # Cost of deletions\n",
    "                distances[row][col - 1] + 1,         # Cost of insertions\n",
    "                distances[row - 1][col - 1] + cost   # Cost of substitutions\n",
    "            )\n",
    "\n",
    "    pprint(distances)\n",
    "    print()\n",
    "\n",
    "    return distances[row][col]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. [5] Menggunakan fungsi yang telah dibuat pada soal sebelumnya, carilah nilai edit_distance dari pasangan kata berikut ini:\n",
    "\n",
    "### a. phasmophobia - puafnilhotik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n",
      "       [ 1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
      "       [ 2,  1,  1,  2,  3,  4,  5,  6,  6,  7,  8,  9, 10],\n",
      "       [ 3,  2,  2,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
      "       [ 4,  3,  3,  2,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
      "       [ 5,  4,  4,  3,  3,  3,  4,  5,  6,  7,  8,  9, 10],\n",
      "       [ 6,  5,  5,  4,  4,  4,  4,  5,  6,  6,  7,  8,  9],\n",
      "       [ 7,  6,  6,  5,  5,  5,  5,  5,  6,  7,  7,  8,  9],\n",
      "       [ 8,  7,  7,  6,  6,  6,  6,  6,  5,  6,  7,  8,  9],\n",
      "       [ 9,  8,  8,  7,  7,  7,  7,  7,  6,  5,  6,  7,  8],\n",
      "       [10,  9,  9,  8,  8,  8,  8,  8,  7,  6,  6,  7,  8],\n",
      "       [11, 10, 10,  9,  9,  9,  8,  9,  8,  7,  7,  6,  7],\n",
      "       [12, 11, 11, 10, 10, 10,  9,  9,  9,  8,  8,  7,  7]])\n",
      "\n",
      "Leivenshtein distance antara kedua kata di atas adalah 7\n"
     ]
    }
   ],
   "source": [
    "leivenshtein_distance = edit_distance(\"phasmophobia\", \"puafnilhotik\")\n",
    "\n",
    "print(f\"Leivenshtein distance antara kedua kata di atas adalah {leivenshtein_distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. genetik - ganteng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 1, 2, 3, 4, 5, 6, 7],\n",
      "       [1, 0, 1, 2, 3, 4, 5, 6],\n",
      "       [2, 1, 1, 2, 3, 3, 4, 5],\n",
      "       [3, 2, 2, 1, 2, 3, 3, 4],\n",
      "       [4, 3, 3, 2, 2, 2, 3, 4],\n",
      "       [5, 4, 4, 3, 2, 3, 3, 4],\n",
      "       [6, 5, 5, 4, 3, 3, 4, 4],\n",
      "       [7, 6, 6, 5, 4, 4, 4, 5]])\n",
      "\n",
      "Leivenshtein distance antara kedua kata di atas adalah 5\n"
     ]
    }
   ],
   "source": [
    "leivenshtein_distance = edit_distance(\"genetik\", \"ganteng\")\n",
    "\n",
    "print(f\"Leivenshtein distance antara kedua kata di atas adalah {leivenshtein_distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. [13] Salah satu metode untuk menghitung edit distance secara heuristik adalah menggunakan “keyboard distance”. Penentuan “bobot” atau “weight” pada metode ini dapat bervariasi. Penggunaan graf dari keyboard (asumsikan QWERTY keyboard) merupakan salah satu metode yang dapat dilakukan.\n",
    "\n",
    "### Pada graf yang akan digunakan, satu node merepresentasikan satu key. Kemudian, satu edge menandakan bahwa dua key berdekatan yang artinya memiliki bobot edit distance sebesar 1.\n",
    "\n",
    "### Buatlah matriks edit distance M dari suatu QWERTY keyboard! M(ii, jj) menyatakan jarak terpendek antara huruf “ii” ke huruf “jj” dalam graf QWERTY keyboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwerty_keyboard = [\n",
    "    ['q', 'w', 'e', 'r', 't', 'y', 'u', 'i', 'o', 'p'],\n",
    "    ['a', 's', 'd', 'f', 'g', 'h', 'j', 'k', 'l'],\n",
    "    [' ', 'z', 'x', 'c', 'v', 'b', 'n', 'm', ' '],\n",
    "]\n",
    "\n",
    "alphabet = string.ascii_lowercase\n",
    "\n",
    "def get_coordinate(char, array):\n",
    "    x, y = -1, -1\n",
    "    for row in array:\n",
    "        if char in row:\n",
    "            x = array.index(row)\n",
    "            y = row.index(char)\n",
    "            return (x, y)\n",
    "    raise IndexError()\n",
    "\n",
    "def M(ii, jj):\n",
    "    \"\"\"\n",
    "    Asumsi M(ii, jj) adalah fungsi untuk mengembalikan jarak\n",
    "    antara karakter ii dan jj\n",
    "    \"\"\"\n",
    "    coordinate_1 = get_coordinate(ii, qwerty_keyboard)\n",
    "    coordinate_2 = get_coordinate(jj, qwerty_keyboard)\n",
    "    return round(\n",
    "        ((coordinate_1[0] - coordinate_2[0])**2 + (coordinate_1[1] - coordinate_2[1])**2)**(0.5)\n",
    "    )\n",
    "\n",
    "def count_edit_distance(str_1, str_2):\n",
    "    edit_distance_sum = 0\n",
    "\n",
    "    for i in range(len(str_1)):\n",
    "        distance = M(str_1[i], str_2[i])\n",
    "        if distance:\n",
    "            edit_distance_sum += distance\n",
    "\n",
    "    return edit_distance_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. [5] Menggunakan matriks M yang telah dibuat pada soal sebelumnya, carilah nilai edit_distance dari pasangan kata berikut ini:\n",
    "\n",
    "### a. phasmophobia - puafnilhotik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard edit distance antara kedua kata di atas adalah 15\n"
     ]
    }
   ],
   "source": [
    "keyboard_edit_distance = count_edit_distance('phasmophobia', 'puafnilhotik')\n",
    "\n",
    "print(f\"Keyboard edit distance antara kedua kata di atas adalah {keyboard_edit_distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. genetik - ganteng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1087,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard edit distance antara kedua kata di atas adalah 11\n"
     ]
    }
   ],
   "source": [
    "keyboard_edit_distance = count_edit_distance('genetik', 'ganteng')\n",
    "\n",
    "print(f\"Keyboard edit distance antara kedua kata di atas adalah {keyboard_edit_distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. [7] Bandingkan hasil yang kamu temui pada nomor A2 dan A4. Apa yang dapat kamu simpulkan terkait kedua metode edit distance ini? Selain itu, apakah kedua algoritma edit distance ini sudah sempurna? Jelaskan analisis singkat kamu minimal dalam 3 kalimat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apa yang dapat kamu simpulkan terkait kedua metode edit distance ini?**\n",
    "\n",
    "Kedua metode adalah pendekatan yang digunakan apabila ingin mendeteksi error pada teks dengan tipe Non-word errors. Yakni pendekatan yang lebih banyak digunakan dalam kasus kesalahan penulisan yang mengakibatkan kata menjadi tidak bermakna.\n",
    "\n",
    "**Apakah kedua algoritma edit distance ini sudah sempurna?**\n",
    "\n",
    "Belum, karena kedua algoritma di atas belum bisa mendeteksi error yang bertipe real-word errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **B - N-Gram Language Model (30 Poin)**\n",
    "\n",
    "### Perhatikan empat kalimat berikut\n",
    "### K1 : orang itu menjadi kepala rumah sakit di desa\n",
    "### K2 : kepala orang itu sedang sakit di rumah sakit\n",
    "### K3 : kepala rumah sakit itu memakan kelapa bersama warga desa\n",
    "### K4 : orang sakit itu memakan kelapa bersama kepala rumah sakit\n",
    "\n",
    "### 1. [12] Jika diberikan threshold sebesar 10^-12 dengan menggunakan language model bigram dan trigram, manakah di antara kalimat berikut ini yang memiliki kecenderungan pada spelling error? Tunjukkan peluang dari setiap kalimat berikut dan tentukan kalimat yang mengandung spelling error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['orang', 'itu', 'menjadi', 'kepala', 'rumah', 'sakit', 'di', 'desa'], ['kepala', 'orang', 'itu', 'sedang', 'sakit', 'di', 'rumah', 'sakit'], ['kepala', 'rumah', 'sakit', 'itu', 'memakan', 'kelapa', 'bersama', 'warga', 'desa'], ['orang', 'sakit', 'itu', 'memakan', 'kelapa', 'bersama', 'kepala', 'rumah', 'sakit']] \n",
      "\n",
      "[[('orang', 'itu'), ('itu', 'menjadi'), ('menjadi', 'kepala'), ('kepala', 'rumah'), ('rumah', 'sakit'), ('sakit', 'di'), ('di', 'desa')], [('kepala', 'orang'), ('orang', 'itu'), ('itu', 'sedang'), ('sedang', 'sakit'), ('sakit', 'di'), ('di', 'rumah'), ('rumah', 'sakit')], [('kepala', 'rumah'), ('rumah', 'sakit'), ('sakit', 'itu'), ('itu', 'memakan'), ('memakan', 'kelapa'), ('kelapa', 'bersama'), ('bersama', 'warga'), ('warga', 'desa')], [('orang', 'sakit'), ('sakit', 'itu'), ('itu', 'memakan'), ('memakan', 'kelapa'), ('kelapa', 'bersama'), ('bersama', 'kepala'), ('kepala', 'rumah'), ('rumah', 'sakit')]] \n",
      "\n",
      "[[('orang', 'itu', 'menjadi'), ('itu', 'menjadi', 'kepala'), ('menjadi', 'kepala', 'rumah'), ('kepala', 'rumah', 'sakit'), ('rumah', 'sakit', 'di'), ('sakit', 'di', 'desa')], [('kepala', 'orang', 'itu'), ('orang', 'itu', 'sedang'), ('itu', 'sedang', 'sakit'), ('sedang', 'sakit', 'di'), ('sakit', 'di', 'rumah'), ('di', 'rumah', 'sakit')], [('kepala', 'rumah', 'sakit'), ('rumah', 'sakit', 'itu'), ('sakit', 'itu', 'memakan'), ('itu', 'memakan', 'kelapa'), ('memakan', 'kelapa', 'bersama'), ('kelapa', 'bersama', 'warga'), ('bersama', 'warga', 'desa')], [('orang', 'sakit', 'itu'), ('sakit', 'itu', 'memakan'), ('itu', 'memakan', 'kelapa'), ('memakan', 'kelapa', 'bersama'), ('kelapa', 'bersama', 'kepala'), ('bersama', 'kepala', 'rumah'), ('kepala', 'rumah', 'sakit')]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'orang itu menjadi kepala rumah sakit di desa',\n",
    "    'kepala orang itu sedang sakit di rumah sakit',\n",
    "    'kepala rumah sakit itu memakan kelapa bersama warga desa',\n",
    "    'orang sakit itu memakan kelapa bersama kepala rumah sakit',\n",
    "]\n",
    "\n",
    "unigrams = [\n",
    "    row.split(' ') for row in sentences\n",
    "]\n",
    "\n",
    "bigrams = map(\n",
    "    lambda row: list(nltk.ngrams(row.split(' '), 2)),\n",
    "    sentences\n",
    ")\n",
    "\n",
    "trigrams = map(\n",
    "    lambda row: list(nltk.ngrams(row.split(' '), 3)),\n",
    "    sentences\n",
    ")\n",
    "\n",
    "unigrams_sr = pd.Series(unigrams)\n",
    "bigrams_sr = pd.Series(bigrams)\n",
    "trigrams_sr = pd.Series(trigrams)\n",
    "\n",
    "print(list(unigrams_sr), \"\\n\")\n",
    "print(list(bigrams_sr), \"\\n\")\n",
    "print(list(trigrams_sr), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1089,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sakit      6\n",
      "itu        4\n",
      "kepala     4\n",
      "rumah      4\n",
      "orang      3\n",
      "di         2\n",
      "desa       2\n",
      "memakan    2\n",
      "kelapa     2\n",
      "bersama    2\n",
      "menjadi    1\n",
      "sedang     1\n",
      "warga      1\n",
      "dtype: int64 \n",
      "\n",
      "(rumah, sakit)       4\n",
      "(kepala, rumah)      3\n",
      "(orang, itu)         2\n",
      "(sakit, itu)         2\n",
      "(sakit, di)          2\n",
      "(kelapa, bersama)    2\n",
      "(memakan, kelapa)    2\n",
      "(itu, memakan)       2\n",
      "(orang, sakit)       1\n",
      "(warga, desa)        1\n",
      "(bersama, warga)     1\n",
      "(sedang, sakit)      1\n",
      "(di, rumah)          1\n",
      "(itu, menjadi)       1\n",
      "(itu, sedang)        1\n",
      "(kepala, orang)      1\n",
      "(di, desa)           1\n",
      "(menjadi, kepala)    1\n",
      "(bersama, kepala)    1\n",
      "dtype: int64 \n",
      "\n",
      "(kepala, rumah, sakit)        3\n",
      "(memakan, kelapa, bersama)    2\n",
      "(itu, memakan, kelapa)        2\n",
      "(sakit, itu, memakan)         2\n",
      "(orang, itu, menjadi)         1\n",
      "(di, rumah, sakit)            1\n",
      "(kelapa, bersama, kepala)     1\n",
      "(orang, sakit, itu)           1\n",
      "(bersama, warga, desa)        1\n",
      "(kelapa, bersama, warga)      1\n",
      "(rumah, sakit, itu)           1\n",
      "(sakit, di, rumah)            1\n",
      "(itu, menjadi, kepala)        1\n",
      "(sedang, sakit, di)           1\n",
      "(itu, sedang, sakit)          1\n",
      "(orang, itu, sedang)          1\n",
      "(kepala, orang, itu)          1\n",
      "(sakit, di, desa)             1\n",
      "(rumah, sakit, di)            1\n",
      "(menjadi, kepala, rumah)      1\n",
      "(bersama, kepala, rumah)      1\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "unigrams_sr_explode = unigrams_sr.explode()\n",
    "unigrams_sr_value_counts = unigrams_sr_explode.value_counts()\n",
    "\n",
    "bigrams_sr_explode = bigrams_sr.explode()\n",
    "bigrams_sr_value_counts = bigrams_sr_explode.value_counts()\n",
    "\n",
    "trigrams_sr_explode = trigrams_sr.explode()\n",
    "trigrams_sr_value_counts = trigrams_sr_explode.value_counts()\n",
    "\n",
    "# print(unigrams_sr_explode, \"\\n\")\n",
    "# print(bigrams_sr_explode, \"\\n\")\n",
    "# print(trigrams_sr_explode, \"\\n\")\n",
    "print(unigrams_sr_value_counts, \"\\n\")\n",
    "print(bigrams_sr_value_counts, \"\\n\")\n",
    "print(trigrams_sr_value_counts, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occurence(tup_src, model):\n",
    "    \"\"\"\n",
    "    Count occurences of a n-gram in the model provided\n",
    "\n",
    "    params\n",
    "    tup_src: tuple n-gram yang ingin dicek\n",
    "    model: model yang menjadi referensi\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return model[tup_src]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def count_bigrams_prob(bigrams):\n",
    "    \"\"\"\n",
    "    Count bigrams probability relative to the model di atas\n",
    "\n",
    "    params\n",
    "    bigrams: bigram yang pengen diitung prob-nya\n",
    "    \"\"\"\n",
    "    bigrams_prob = (\n",
    "        count_occurence(bigrams_a[0][0], unigrams_sr_value_counts)/\n",
    "        unigrams_sr_value_counts.size\n",
    "    )\n",
    "\n",
    "    for bigram in bigrams:\n",
    "        count_bigram = count_occurence(bigram, bigrams_sr_value_counts)\n",
    "        count_word = count_occurence(bigram[0], unigrams_sr_value_counts)\n",
    "        count_prob = (count_bigram + 1)/(count_word + len(bigram[0]))\n",
    "\n",
    "        if count_prob:\n",
    "            bigrams_prob *= count_prob\n",
    "\n",
    "    return bigrams_prob\n",
    "\n",
    "def count_trigrams_prob(trigrams):\n",
    "    \"\"\"\n",
    "    Count trigrams probability relative to the model di atas\n",
    "\n",
    "    params\n",
    "    trigrams: trigram yang pengen diitung prob-nya\n",
    "    \"\"\"\n",
    "    trigrams_prob = (\n",
    "        count_occurence(trigrams_a[0][0], unigrams_sr_value_counts)/\n",
    "        unigrams_sr_value_counts.size\n",
    "    )\n",
    "\n",
    "    for trigram in trigrams:\n",
    "        count_trigram = count_occurence(trigram, trigrams_sr_value_counts)\n",
    "        count_word = count_occurence(trigram[0], unigrams_sr_value_counts)\n",
    "        count_prob = (count_trigram + 1)/(count_word + len(trigram[0]))\n",
    "\n",
    "        if count_prob:\n",
    "            trigrams_prob *= count_prob\n",
    "\n",
    "    return trigrams_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. warga desa sedang sakit kelapa di rumah sakit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilitas kalimat dengan menggunakan bigram adalah 3.854170520837187e-06\n",
      "Probabilitas kalimat dengan menggunakan trigram adalah 9.635426302092967e-08\n"
     ]
    }
   ],
   "source": [
    "sentence_a = 'warga desa sedang sakit kelapa di rumah sakit'\n",
    "\n",
    "bigrams_a = list(nltk.ngrams(sentence_a.split(' '), 2))\n",
    "trigrams_a = list(nltk.ngrams(sentence_a.split(' '), 3))\n",
    "\n",
    "bigrams_prob = count_bigrams_prob(bigrams_a)\n",
    "trigrams_prob = count_trigrams_prob(bigrams_a)\n",
    "\n",
    "print(f'Probabilitas kalimat dengan menggunakan bigram adalah {bigrams_prob}')\n",
    "print(f'Probabilitas kalimat dengan menggunakan trigram adalah {trigrams_prob}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. orang itu sedang berada di rumah sakit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilitas kalimat dengan menggunakan bigram adalah 5.450898308041165e-05\n",
      "Probabilitas kalimat dengan menggunakan trigram adalah 9.084830513401942e-07\n"
     ]
    }
   ],
   "source": [
    "sentence_b = 'orang itu sedang berada di rumah sakit'\n",
    "\n",
    "bigrams_b = list(nltk.ngrams(sentence_b.split(' '), 2))\n",
    "trigrams_b = list(nltk.ngrams(sentence_b.split(' '), 3))\n",
    "\n",
    "bigrams_prob = count_bigrams_prob(bigrams_b)\n",
    "trigrams_prob = count_trigrams_prob(bigrams_b)\n",
    "\n",
    "print(f'Probabilitas kalimat dengan menggunakan bigram adalah {bigrams_prob}')\n",
    "print(f'Probabilitas kalimat dengan menggunakan trigram adalah {trigrams_prob}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. rumah sakit di desa itu tempat warga memakan kepala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilitas kalimat dengan menggunakan bigram adalah 4.282411689819096e-07\n",
      "Probabilitas kalimat dengan menggunakan trigram adalah 1.4274705632730318e-08\n"
     ]
    }
   ],
   "source": [
    "sentence_c = 'rumah sakit di desa itu tempat warga memakan kepala'\n",
    "\n",
    "bigrams_c = list(nltk.ngrams(sentence_c.split(' '), 2))\n",
    "trigrams_c = list(nltk.ngrams(sentence_c.split(' '), 3))\n",
    "\n",
    "bigrams_prob = count_bigrams_prob(bigrams_c)\n",
    "trigrams_prob = count_trigrams_prob(bigrams_c)\n",
    "\n",
    "print(f'Probabilitas kalimat dengan menggunakan bigram adalah {bigrams_prob}')\n",
    "print(f'Probabilitas kalimat dengan menggunakan trigram adalah {trigrams_prob}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. orang sakit sedang memakan kelapa bersama warga desa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilitas kalimat dengan menggunakan bigram adalah 2.3125023125023124e-06\n",
      "Probabilitas kalimat dengan menggunakan trigram adalah 3.2118087673643226e-08\n"
     ]
    }
   ],
   "source": [
    "sentence_d = 'orang sakit sedang memakan kelapa bersama warga desa'\n",
    "\n",
    "bigrams_d = list(nltk.ngrams(sentence_d.split(' '), 2))\n",
    "trigrams_d = list(nltk.ngrams(sentence_d.split(' '), 3))\n",
    "\n",
    "bigrams_prob = count_bigrams_prob(bigrams_d)\n",
    "trigrams_prob = count_trigrams_prob(bigrams_d)\n",
    "\n",
    "print(f'Probabilitas kalimat dengan menggunakan bigram adalah {bigrams_prob}')\n",
    "print(f'Probabilitas kalimat dengan menggunakan trigram adalah {trigrams_prob}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kesimpulan:\n",
    "\n",
    "Semua penghitungan n-gram model didapatkan berada di atas treshold, sehingga tidak didapati real-word error pada kelima kalimat di atas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. [8] Dalam kasus umum (tidak terbatas pada soal ini), menurut kamu, manakah yang lebih baik; language model unigram, bigram, atau trigram? Berikan alasan pendukung jawaban kamu minimal dalam 3 kalimat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referensi: https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/\n",
    "\n",
    "Pada kasus kelima kalimat di atas, terlihat bahwa probabilitas pada trigram selalu lebih akurat/lebih teliti. Sehingga dapat diambil kesimpulan bahwa n-gram dengan n yang lebih besar akan berakibat pada hasil probabilitas akan lebih akurat. Hal ini juga didukung pada referensi di atas, yang menyatakan bahwa semakin besar n, maka akan semakin akurat juga pendeteksian error real-word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. [10] Menurut kamu apakah N-gram language model yang standar sudah baik untuk diterapkan pada bahasa yang kaya secara morfologis, seperti Bahasa Indonesia? Apa saja masalah berkaitan dengan morfologis yang mungkin ditemukan pada kasus ini? Ceritakan strategi kamu dalam mengatasinya dikaitkan dengan pengetahuan yang telah didapatkan pada bagian Text Processing minimal dalam 3 kalimat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menurut saya, dan sepengetahuan saya, metode n-gram lebih tepat untuk digunakan pada kasus-kasus seperti auto-complete/prediksi kata selanjutnya. Mengenai permasalahan morfologi, saya rasa hal ini lebih tepat untuk diselesaikan dengan pendekatan stemming & lemmatization sebagaimana yang sempat dikerjakan pada Tugas 1 sebelumnya. Untuk mengatasi masalah ini, mungkin bisa dilakukan/dibuat pipeline IR yang utuh yang juga menyertakan kedua tahapan (apabila dapat dikombinasikan), agar didapatkan hasil informasi/prediksi/*query* yang semakin akurat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **C - Pemetaan Dokumen dan Teks (30 Poin)**\n",
    "\n",
    "### Dokumen A\n",
    "<blockquote>\n",
    "Dokumen A Sivitas Akademika Universitas Indonesia menjalankan aktivitas perkuliahan secara daring yang diikuti oleh mahasiswa dari seluruh jenjang. Hal ini diakibatkan karena adanya pandemi COVID-19 di Indonesia selama satu setengah tahun belakang.\n",
    "</blockquote>\n",
    "\n",
    "### Dokumen B\n",
    "<blockquote>\n",
    "Masyarakat memiliki respon berbeda dalam menyikapi pandemi COVID-19. Salah satu mahasiswa bernama Dipsy mengatakan kesulitan untuk belajar dengan maksimal karena sulit untuk berdiskusi dengan orang yang lebih pandai.\n",
    "</blockquote>\n",
    "\n",
    "### Dokumen C\n",
    "<blockquote>\n",
    "Salah satu kunci untuk menghadapi musibah adalah dengan bersabar dan yakin semua musibah akan berlalu pada waktunya. Badai pasti berlalu, jadi harus tetap semangat.\n",
    "</blockquote>\n",
    "\n",
    "### 1. [10] Buatlah pemetaan seluruh kata yang sudah dalam bentuk kata dasar lowercase dan bukan termasuk dalam stopwords dengan jumlah kemunculan kata tersebut di setiap dokumen. Kamu dibebaskan bagaimana cara menampilkannya, (tidak harus dalam bentuk tabel, bisa saja dalam bentuk key-value pair)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    'Dokumen A Sivitas Akademika Universitas Indonesia menjalankan aktivitas perkuliahan secara daring yang diikuti oleh mahasiswa dari seluruh jenjang. Hal ini diakibatkan karena adanya pandemi COVID-19 di Indonesia selama satu setengah tahun belakang.',\n",
    "    'Masyarakat memiliki respon berbeda dalam menyikapi pandemi COVID-19. Salah satu mahasiswa bernama Dipsy mengatakan kesulitan untuk belajar dengan maksimal karena sulit untuk berdiskusi dengan orang yang lebih pandai.',\n",
    "    'Salah satu kunci untuk menghadapi musibah adalah dengan bersabar dan yakin semua musibah akan berlalu pada waktunya. Badai pasti berlalu, jadi harus tetap semangat.',\n",
    "]\n",
    "\n",
    "documents_sr = pd.Series(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform uncasing\n",
    "cleaned_documents = documents_sr.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations\n",
    "cleaned_documents = cleaned_documents.str.replace('[^\\w\\s]', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize documents\n",
    "cleaned_documents_tokenized = cleaned_documents.map(\n",
    "    lambda document: nltk.word_tokenize(document)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "stopwords = pd.read_csv('stopwords.txt', header=None)\n",
    "\n",
    "cleaned_documents_tokenized_no_sw = cleaned_documents_tokenized.copy(deep=True)\n",
    "\n",
    "for index, document in cleaned_documents_tokenized_no_sw.iteritems():\n",
    "    cleaned_documents_tokenized_no_sw.iloc[index] = list(\n",
    "        filter(\n",
    "            lambda word: word not in stopwords.values,\n",
    "            document\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dokumen',\n",
      " 'a',\n",
      " 'sivitas',\n",
      " 'akademika',\n",
      " 'universitas',\n",
      " 'indonesia',\n",
      " 'menjalankan',\n",
      " 'aktivitas',\n",
      " 'perkuliahan',\n",
      " 'daring',\n",
      " 'diikuti',\n",
      " 'mahasiswa',\n",
      " 'jenjang',\n",
      " 'diakibatkan',\n",
      " 'pandemi',\n",
      " 'covid',\n",
      " '19',\n",
      " 'indonesia']\n"
     ]
    }
   ],
   "source": [
    "# Menampikan dokumen yang sudah ditokenisasi dan diproses\n",
    "pprint(cleaned_documents_tokenized_no_sw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['masyarakat',\n",
      " 'memiliki',\n",
      " 'respon',\n",
      " 'berbeda',\n",
      " 'menyikapi',\n",
      " 'pandemi',\n",
      " 'covid',\n",
      " '19',\n",
      " 'salah',\n",
      " 'mahasiswa',\n",
      " 'bernama',\n",
      " 'dipsy',\n",
      " 'kesulitan',\n",
      " 'belajar',\n",
      " 'maksimal',\n",
      " 'sulit',\n",
      " 'berdiskusi',\n",
      " 'orang',\n",
      " 'pandai']\n"
     ]
    }
   ],
   "source": [
    "pprint(cleaned_documents_tokenized_no_sw[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['salah',\n",
      " 'kunci',\n",
      " 'menghadapi',\n",
      " 'musibah',\n",
      " 'bersabar',\n",
      " 'musibah',\n",
      " 'badai',\n",
      " 'semangat']\n"
     ]
    }
   ],
   "source": [
    "pprint(cleaned_documents_tokenized_no_sw[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "musibah        2\n",
      "indonesia      2\n",
      "19             2\n",
      "covid          2\n",
      "pandemi        2\n",
      "salah          2\n",
      "mahasiswa      2\n",
      "dokumen        1\n",
      "maksimal       1\n",
      "dipsy          1\n",
      "kesulitan      1\n",
      "belajar        1\n",
      "berdiskusi     1\n",
      "sulit          1\n",
      "orang          1\n",
      "pandai         1\n",
      "kunci          1\n",
      "menghadapi     1\n",
      "bersabar       1\n",
      "badai          1\n",
      "bernama        1\n",
      "respon         1\n",
      "menyikapi      1\n",
      "berbeda        1\n",
      "a              1\n",
      "memiliki       1\n",
      "masyarakat     1\n",
      "diakibatkan    1\n",
      "jenjang        1\n",
      "diikuti        1\n",
      "daring         1\n",
      "perkuliahan    1\n",
      "aktivitas      1\n",
      "menjalankan    1\n",
      "universitas    1\n",
      "akademika      1\n",
      "sivitas        1\n",
      "semangat       1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Menampilkan pemetaan kata dan jumlah kata\n",
    "cleaned_documents_word_counts = cleaned_documents_tokenized_no_sw.explode()\n",
    "cleaned_documents_word_counts = cleaned_documents_word_counts.value_counts()\n",
    "\n",
    "print(cleaned_documents_word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. [10] Jika menjalankan masing-masing query berikut, dokumen manakah yang akan ditemukan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENTS_INDEX_DICT = {\n",
    "    0: 'A',\n",
    "    1: 'B',\n",
    "    2: 'C',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. akibat AND sulit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query tidak ditemukan dari dokumen manapun\n"
     ]
    }
   ],
   "source": [
    "doc_index = 0\n",
    "is_result_found = False\n",
    "\n",
    "for document in cleaned_documents_tokenized_no_sw:\n",
    "    if (('akibat' in document) and ('sulit' in document)):\n",
    "        print(f'Dokumen {DOCUMENTS_INDEX_DICT[doc_index]}')\n",
    "        is_result_found = True\n",
    "    doc_index += 1\n",
    "\n",
    "if not is_result_found:\n",
    "    print(\"Query tidak ditemukan dari dokumen manapun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. waktu OR santai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query tidak ditemukan dari dokumen manapun\n"
     ]
    }
   ],
   "source": [
    "doc_index = 0\n",
    "is_result_found = False\n",
    "\n",
    "for document in cleaned_documents_tokenized_no_sw:\n",
    "    if (('waktu' in document) or ('santai' in document)):\n",
    "        print(f'Dokumen {DOCUMENTS_INDEX_DICT[doc_index]}')\n",
    "        is_result_found = True\n",
    "    doc_index += 1\n",
    "\n",
    "if not is_result_found:\n",
    "    print(\"Query tidak ditemukan dari dokumen manapun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. (masyarakat AND mahasiswa) OR sabar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokumen B\n"
     ]
    }
   ],
   "source": [
    "doc_index = 0\n",
    "is_result_found = False\n",
    "\n",
    "for document in cleaned_documents_tokenized_no_sw:\n",
    "    if ((('masyarakat' in document) and ('mahasiswa' in document)) or ('sabar') in document):\n",
    "        print(f'Dokumen {DOCUMENTS_INDEX_DICT[doc_index]}')\n",
    "        is_result_found = True\n",
    "    doc_index += 1\n",
    "\n",
    "if not is_result_found:\n",
    "    print(\"Query tidak ditemukan dari dokumen manapun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. (libur OR respon) OR (waktu AND jenjang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokumen B\n"
     ]
    }
   ],
   "source": [
    "doc_index = 0\n",
    "is_result_found = False\n",
    "\n",
    "for document in cleaned_documents_tokenized_no_sw:\n",
    "    if ((('libur' in document) or ('respon' in document)) or\n",
    "        (('waktu') in document) and ('jenjang' in document)):\n",
    "        print(f'Dokumen {DOCUMENTS_INDEX_DICT[doc_index]}')\n",
    "        is_result_found = True\n",
    "    doc_index += 1\n",
    "\n",
    "if not is_result_found:\n",
    "    print(\"Query tidak ditemukan dari dokumen manapun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. (kunci OR musibah) AND NOT (aktivitas AND pandemi) OR maksimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokumen B\n"
     ]
    }
   ],
   "source": [
    "doc_index = 0\n",
    "is_result_found = False\n",
    "\n",
    "for document in cleaned_documents_tokenized_no_sw:\n",
    "    if ((('kunci' in document) or ('musibah' in document)) and not\n",
    "        (('aktivitas') in document) and ('pandemi' in document) or\n",
    "        ('maksimal') in document):\n",
    "        print(f'Dokumen {DOCUMENTS_INDEX_DICT[doc_index]}')\n",
    "        is_result_found = True\n",
    "    doc_index += 1\n",
    "\n",
    "if not is_result_found:\n",
    "    print(\"Query tidak ditemukan dari dokumen manapun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. [10] Apakah metode pemetaan dokumen dan teks dapat memberikan manfaat dalam proses perancangan sistem IR apabila dilihat dari perspektif keakuratan hasil yang akan dihasilkan? Jelaskan analisis singkat kamu minimal dalam 3 kalimat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apabila meninjau dari perspektif keakuratan, menurut pemahaman saya yang masih sedikit ini, pemetaan dokumen dan teks tidak begitu berdampak (atau tidak memiliki dampak yang begitu signifikan) dengan keakuratan hasil dan *query*. Akan tetapi, akan lebih berdampak kepada performa pencarian yang menjadi lebih cepat mengingat kemunculan kata dan dokumen sudah dimapping. Setelah mengerjakan subnomor ini, saya jadi merasa ragu dengan pengerjaan saya pada nomor C1, pasalnya saya hanya melakukan mapping antara kata dengan occurencesnya, dan tidak mapping pada dokumen apa saja kata tersebut muncul."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
